NLA-396
status: in progress
when done, needs to be incorpotated in Bal2017dekk0505.xml (?)

= LMNL Import

How do we map the lmnl syntax to our TAG data model?
We made a lexer grammar for the LMNL syntax and used ANTLR4 to create the basis of a lexer for LMNL.
This lexer basically breaks the LMNL text up into tokens, and the lexer can switch between modes based on the current node and token, and set the type of the token.
This stream of tokens is then parsed in the importer, where the token type is used to determine in which part of the LMNL we are: in the open or close tag of a range or annotation, inside an annotation, or in the text.
Because in LMNL annotations can be nested: annotations inside annotation openers, or annotations inside range opener/closers inside annotation text, the importer needs to keep track of the layer it's currently in: either in the base text layer, or inside an annotation.
At the start of the import, we create a new Document, which serves as the default text layer.
Then after each range start tag we create a new Markup, add this Markup to the list of open Markups, and to the Document.
For each text token we create a new TextNode, we link this TextNode to all the open Markups, connect it to the previous TextNode, and add it to the Document.
After each range end tag we remove the corresponding Markup from the list of open Markups.
For each annotation start tag encountered we create a new Annotation, and add this to the annotation list of the current Markup. Unless the annotation is empty, we now set this Annotation as the current text layer, so until we come to the annotation close tag for this annotation, all new TextNodes and Markup will be added to this annotation. Then when we've encountered the corresponding annotation end tag, we close this Annotation and go back to the previous text layer.

= TexMECS Import

For importing TexMECS we've also written an ANTLR4 grammar, this time both a lexer and a parser grammar, since TexMECS doesn't have the nesting that LMNL has.
The TexMECS importer is based on the parser generated by ANTLR4.
At the start of the import, we create a new Document.
After each start tag, we create a new Markup, add this Markup to the list of open Markups, and to the Document.
For each text token we create a new TextNode, we link this TextNode to all the open Markups, connect it to the previous TextNode, and add it to the Document.
After each end tag we remove the corresponding Markup from the list of open Markups.
After each suspend tag we remove the corresponding Markup from the list of open Markups, and add it to the list of suspended Markups.
After each resume tag we remove the corresponding Markup from the list of suspended Markups, and add it to the list of opened Markups.

= TAGQL

= Alexandria Markup Server API

The Alexandria Markup server has a REST API:

GET     /documents
  (out: json) - returns a list of the urls of the stored documents

POST    /documents/lmnl
  (in: lmnl text) add a document using a lmnl text, returns the id of the document in the Location header

POST    /documents/texmecs
  (in: texmecs text) add a document using a TexMECS text, returns the id of the document in the Location header

GET     /documents/{uuid}
  (out: json) returns information about the document

GET     /documents/{uuid}/lmnl
  (out: text) returns a lmnl representation of the document

GET     /documents/{uuid}/latex
  (out: latex text) returns a visualization of the main document layer as text nodes and markup nodes

GET     /documents/{uuid}/markupdepth
  (out: latex text) returns the main text nodes of the document, color-coded for the number of different markup nodes per text node.

GET     /documents/{uuid}/matrix
  (out: latex text) returns the optimized text node / markup matrix of the document

GET     /documents/{uuid}/kdtree
  (out: latex text) returns the kd-tree of the document

POST    /documents/{uuid}/query
  (in: text, out: json) execute a query on the document, return results as json

= Java/Python clients
